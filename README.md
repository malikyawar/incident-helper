# ğŸ› ï¸ Incident Helper CLI

## An AI-powered terminal assistant for SREs and DevOps engineers
Triage incidents, interpret logs, and debug systems, right from your terminal, powered by local LLMs like Mistral (via Ollama).


## ğŸš€ Features

- ğŸ” Accepts natural-language incident reports
- ğŸ’¬ Conversational CLI: ask â†’ guide â†’ suggest â†’ fix
- ğŸ§  Remembers session context (alert, OS, answers)
- ğŸ“‚ Helps with logs, system metrics, SSH access
- ğŸ§© AI model plug-in ready (Ollama, OpenAI, Claude)
- ğŸ Python CLI built with Typer
- ğŸ”Œ Extensible design with plugin-ready resolvers


## ğŸ“¦ Prerequisites

- Python 3.8+
- [Ollama](https://ollama.com) installed and a model pulled (e.g. `ollama run mistral`)
- Git & pip


## âš™ï¸ Installation

```bash
git clone https://github.com/yourname/incident-helper.git
cd incident-helper
pip install -e .
````

## ğŸ’¡ How It Works

* Users start the tool with `incident-helper start`
* The CLI prompts for incident context.
* The AI (via Ollama) processes inputs and suggests diagnostic steps.
* Maintains context across the full session.
* Guides log checks, SSH access, service status, etc.
* Suggests real commands and interprets output depending on your system. 


## ğŸ§  Key Features

* ğŸ’¬ Conversational interface.
* ğŸ§  Session memory across turns.
* ğŸ§© Plugin support for local or remote AI models.  
* ğŸ”Œ Extensible architecture for adding new â€œresolversâ€ (logs, SSH, etc.)
* ğŸ”® More AI models (Claude, OpenAI, etc.) will be supported in future releases via the pluggable LLM engine.


## ğŸ§  Usage

```bash
$ incident-helper start

ğŸ‘‹ Hi! Describe your incident.
> Getting 5xx on AWS ELB

ğŸ¤– What OS are you using?
> Amazon Linux 2

ğŸ¤– Can you access the logs? Where are they?
> /var/log/nginx/error.log

ğŸ¤– Great. Try: less /var/log/nginx/error.log
```

Type `exit` anytime to end the session.


## ğŸ“ License

MIT
